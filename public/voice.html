<!DOCTYPE html>
<html lang="es">
<head>
<meta charset="UTF-8">
<title>CONSIA CORE</title>

<script src="https://cdn.jsdelivr.net/npm/three@0.160.0/build/three.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/three@0.160.0/examples/js/loaders/GLTFLoader.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@picovoice/porcupine-web@1.2.0/dist/iife/index.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>

<style>
body{
  margin:0;
  background:black;
  overflow:hidden;
}
#canvas3d{
  position:absolute;
  top:0;
  left:0;
}
#video{
  position:absolute;
  bottom:20px;
  right:20px;
  width:180px;
  border-radius:10px;
}
</style>
</head>

<body>

<canvas id="canvas3d"></canvas>
<video id="video" autoplay muted playsinline></video>

<script>
let scene, camera, renderer, avatar;
let socket;
let porcupine;
let audioContext;
let mediaStream;

// THREE.JS INIT
function init3D(){

  scene = new THREE.Scene();

  camera = new THREE.PerspectiveCamera(
    75,
    window.innerWidth/window.innerHeight,
    0.1,
    1000
  );

  renderer = new THREE.WebGLRenderer({
    canvas:document.getElementById("canvas3d"),
    alpha:true
  });

  renderer.setSize(
    window.innerWidth,
    window.innerHeight
  );

  const light = new THREE.PointLight(0x00ffff,2);
  light.position.set(2,2,2);
  scene.add(light);

  const loader = new THREE.GLTFLoader();

  loader.load("/assets/avatar.glb",gltf=>{
    avatar = gltf.scene;
    avatar.position.set(0,-1,-3);
    scene.add(avatar);
  });

  animate();
}

function animate(){
  requestAnimationFrame(animate);
  if(avatar){
    avatar.rotation.y += 0.002;
  }
  renderer.render(scene,camera);
}

// SOCKET REALTIME
function connectSocket(){
  socket = new WebSocket("wss://api.consia.world/realtime");

  socket.onopen = ()=>{
    console.log("Socket conectado");
  };

  socket.onmessage = msg=>{
    console.log("Server:",msg.data);
  };
}

// STREAM AUDIO
async function startStreaming(){

  mediaStream =
    await navigator.mediaDevices.getUserMedia({audio:true});

  const recorder =
    new MediaRecorder(mediaStream);

  recorder.start(300);

  recorder.ondataavailable = e=>{
    if(socket.readyState===1){
      socket.send(e.data);
    }
  };
}

// WAKE WORD
async function initWake(){

  const accessKey = 9+cUNbpO4ENSiqX0sPEMphHDjK3cRgd9oWJwMnzSJ8l9o0R/x8k82Q==

  porcupine =
    await PorcupineWeb.PorcupineWorker.create(
      accessKey,
      [{
        builtin:"Computer",
        sensitivity:0.7
      }],
      wakeDetected
    );

  audioContext = new AudioContext();

  const mic =
    await navigator.mediaDevices.getUserMedia({audio:true});

  const source =
    audioContext.createMediaStreamSource(mic);

  const processor =
    audioContext.createScriptProcessor(512,1,1);

  source.connect(processor);
  processor.connect(audioContext.destination);

  processor.onaudioprocess = e=>{
    porcupine.process(
      e.inputBuffer.getChannelData(0)
    );
  };
}

function wakeDetected(){
  console.log("WAKE");
  startStreaming();
}

// FACE TRACKING
function initFaceTracking(){

  const video =
    document.getElementById("video");

  const faceMesh =
    new FaceMesh({
      locateFile:file=>{
        return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`;
      }
    });

  faceMesh.setOptions({
    maxNumFaces:1,
    refineLandmarks:true,
    minDetectionConfidence:0.5,
    minTrackingConfidence:0.5
  });

  faceMesh.onResults(results=>{
    if(results.multiFaceLandmarks.length>0){
      if(avatar){
        avatar.rotation.y =
          results.multiFaceLandmarks[0][1].x - 0.5;
      }
    }
  });

  const cam =
    new Camera(video,{
      onFrame:async()=>{
        await faceMesh.send({image:video});
      },
      width:640,
      height:480
    });

  cam.start();
}

// INIT ALL
init3D();
connectSocket();
initWake();
initFaceTracking();

</script>
</body>
</html>
